<!DOCTYPE html>

<html lang="en">

<head>
        <meta charset="utf-8">
        <title>FAQ | Virtual Light Table</title>
        <link rel="stylesheet" href="./style.css">
        <link rel="shortcut icon" type="image/x-icon" href="./src/logo.ico">
        <script src="./src/jquery-3.6.0.min.js"></script>
</head>
<body>
    <div class="main-text-area">
        <div class="home"></div>
        <img src="./src/VLT_logo_broad.png" class="sublogo">
        <h1>F.A.Q.</h1>
        <h2>Frequently Asked Questions</h2>

        <h3>Measurements</h3>

        <div class="faq-item">
            <div class="question">
                <h4>My measured distance values don't correspond to the image?</h4>
            </div>
            <div class="answer collapsed">
                <p class="issue">Issue: When conducting reconstructions and taking measurements, such as with the measurement tool or by reading numbers from a scale or ruler, you may encounter discrepancies between the measured distances and the image content. Even if a ruler is present in the image, measuring 1cm on it may yield different values on the VLT (Virtual Laboratory Toolkit). How is this possible?</p>
                <p class="solution">Solution: To address this, ensure that you have the correct Pixels Per Inch (PPI) set for your image. If uncertain, edit the image (right-click and select "edit" or press O) and adjust the PPI. You can either manually enter the values if known, or use the ppi-measurement tool to automatically compute the resolution. Simply select the tool and choose two points in the image that are exactly 1cm apart from each other.</p>
                <p class="explanation">Explanation: Many images contain internal EXIF data that stores information about their resolution. The VLT first attempts to read this data to streamline your work. However, the resolution of the recording device (e.g., camera or scanner) does not necessarily match the resolution of the depicted objects. To ensure accurate scaling of your objects, you need a ruler or scale as a reference for 1cm in the image. The mentioned ppi-measurement tool allows you to automatically compute the object's resolution, ensuring precise measurements.</p>
            </div>
        </div>

        <h3>Machine Learning</h3>

        <div class="faq-item">
            <div class="question">
                <h4>Can I Use Automatic Segmentation on my Ostraka/Reliefs/Stones?</h4>
            </div>
            <div class="answer collapsed">
                <p class="issue">While designed in a papyrological project, the VLT is in general applicable to a whole range of (two-dimensional) fragmentary objects. Therefore, it would also be helpful if there were a method for automatic segmentation for other types of materials as well.</p>
                <p class="explanation">Unfortunately, this is currently not possible. The machines trained for the VLT so far are all based on data annotations for ancient Egyptian papyri. However, in general it is no problem to train other machines for different types of material IF there is annotated training data available. If you want to contribute by creating your own training data, leading to a segmentation machine for your material, don't hesitate to <a href="./contact.html">contact me</a>.</p>
            </div>
        </div>

        <div class="faq-item">
            <div class="question">
                <h4>Which Machine Learning Model Should I Use for my Segmentation?</h4>
            </div>
            <div class="answer collapsed">
                <p class="explanation">There is no clear answer to that. In the course of my doctoral studies I have trained a couple of different machines to segment papyri and ink, also to analyse their capabilities. Machines that provide at least halfway reliable results are thus provided here as a source of potential support in your workflow. It might be worthwhile to test around with them a bit, though - sometimes different machines yield surprisingly different results.</p>
            </div>
        </div>

        <div class="faq-item">
            <div class="question">
                <h4>The Segmentation Result is not Good at All! What Could be Done?</h4>
            </div>
            <div class="answer collapsed">
                <p class="issue">Sometimes the segmentation results for papyri turn out quite bad - parts of the papyri are not recognised, instead other elements of the image have been misread as papyrus. Part of the ink has not been seen. Why is this the case? And how can this be mitigated?</p>
                <p class="explanation">Training a machine learning model requires a large amount of manually annotated training data. For the course of my dissertation a colleague of mine and I have drawn masks for 24 different papyri which have been used to adapt the machine learning models to the papyrus setting. It is clear, however, that due to this rather scarce training material there are many configurations of papyri, writing, and backgrounds the machine has never seen before. Also, sometimes the machine has quite some problems with objects that very much look like papyrus, even though it's not - wooden frames are such an example. Therefore, the automatic segmentation may be a useful support tool, but there may be cases where even with minor corrections the result is not to your liking. In that case, you would have to fall back to a manual drawing with a digital image editing tool. In order to make the machines more robust and stable, the best way would be to have more training material available. If you are willing to provide such data, please don't hesitate to <a href="./contact.html">contact me</a>.</p>
            </div>
        </div>



        <h3>Development</h3>

        <div class="faq-item">
            <div class="question">
                <h4>Can I suggest new features/ideas for the Virtual Light Table?</h4>
            </div>
            <div class="answer collapsed">
                <p class="explanation">Yes, absolutely! I would be very happy for suggestions, ideas, and concepts on how to make the Virtual Light Table even more useful for a broad range of scholars. Please head to the <a href="./contact.html">contact section</a> to write to me directly or create a <a class="extern" href="https://github.com/Stephan-M-Unter/Virtual-Light-Table/issues" target="_blank">Github issue</a> to address some problem or idea you have regarding the software. Please be aware, however, that at the moment further development of the Virtual Light Table has secondary priority as I have to get done with my ongoing PhD thesis. This might result in longer development circles.</p>
            </div>
        </div>
    </div>
    <script src="./script.js"></script>
</body>
</html>